# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

import gensim
from gensim.models import Word2Vec
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

print("\nðŸ”§ 10. WORD EMBEDDINGS")
print("="*50)

class WordEmbeddingsDemo:
    def __init__(self):
        self.sentences = []

    def prepare_training_data(self, documents):
        """Prepare data for Word2Vec training"""
        prepared_sentences = []
        for doc in documents:
            # Tokenize and clean
            # Assuming word_tokenize is available from a previous cell or import
            # from nltk.tokenize import word_tokenize # Uncomment if needed
            tokens = word_tokenize(doc.lower())
            tokens = [token for token in tokens if token.isalpha()]
            prepared_sentences.append(tokens)
        return prepared_sentences

    def train_word2vec(self, sentences, vector_size=100, window=5, min_count=1):
        """Train Word2Vec model"""
        print("Training Word2Vec model...")
        model = Word2Vec(
            sentences=sentences,
            vector_size=vector_size,
            window=window,
            min_count=min_count,
            workers=4,
            sg=1  # Skip-gram
        )
        return model

    def demonstrate_word_relationships(self, model):
        """Show word relationships and analogies"""
        print("\nWORD RELATIONSHIPS:")
        print("-" * 50)

        # Find similar words
        test_words = ['helmet', 'speeding', 'signal', 'riding']

        for word in test_words:
            if word in model.wv:
                similar_words = model.wv.most_similar(word, topn=5)
                print(f"Words similar to '{word}':")
                for similar, score in similar_words:
                    print(f"  {similar}: {score:.3f}")
                print()

        # Word analogies
        print("WORD ANALOGIES:")
        try:
            # helmet : protection :: ? : prevention
            # analogy = model.wv.most_similar(
            #     positive=['protection', 'speeding'],
            #     negative=['helmet']
            # )
            # The above analogy might not work well with the small dataset.
            # Let's try a simpler analogy if possible with the available words.
            # Example: riding - motorcycle + car = ? (might not be meaningful with small data)
            # Let's stick to the original analogy for now, but be aware it might fail.
            analogy = model.wv.most_similar(
                positive=['protection', 'speeding'],
                negative=['helmet']
            )
            print("helmet : protection :: speeding : ?")
            for word, score in analogy:
                print(f"  {word}: {score:.3f}")
        except Exception as e:
            print(f"Could not perform analogy: {e}")
            print("Insufficient data for this specific analogy or words not in vocabulary.")


    def visualize_embeddings(self, model, words):
        """Visualize word embeddings in 2D space"""
        print("\nVISUALIZING WORD EMBEDDINGS...")

        # Get vectors for specified words
        word_vectors = []
        valid_words = []

        for word in words:
            if word in model.wv:
                word_vectors.append(model.wv[word])
                valid_words.append(word)

        if len(word_vectors) < 3:
            print("Not enough words for visualization (need at least 3)")
            return

        # Convert list of vectors to NumPy array
        word_vectors_array = np.array(word_vectors)

        # Reduce dimensionality with t-SNE
        # Perplexity must be less than the number of samples.
        perplexity_value = min(5, len(valid_words) - 1)
        if perplexity_value <= 0:
             print("Cannot perform t-SNE with less than 2 valid words for perplexity calculation.")
             return


        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
        word_vectors_2d = tsne.fit_transform(word_vectors_array)

        # Create plot
        plt.figure(figsize=(12, 8))
        plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])

        for i, word in enumerate(valid_words):
            plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),
                        xytext=(5, 2), textcoords='offset points')

        plt.title('Word Embeddings Visualization (t-SNE)')
        plt.xlabel('Dimension 1')
        plt.ylabel('Dimension 2')
        plt.grid(True, alpha=0.3)
        plt.show()

    def demonstrate_sentence_embeddings(self, model, sentences):
        """Show how to create sentence embeddings"""
        print("\nSENTENCE EMBEDDINGS:")
        print("-" * 50)

        for i, sentence in enumerate(sentences[:3]):
            # Assuming word_tokenize is available
            # from nltk.tokenize import word_tokenize # Uncomment if needed
            tokens = word_tokenize(sentence.lower())
            tokens = [token for token in tokens if token.isalpha() and token in model.wv]

            if tokens:
                # Average of word vectors
                sentence_vector = np.mean([model.wv[token] for token in tokens], axis=0)
                print(f"Sentence {i+1}: {sentence}")
                print(f"Embedding shape: {sentence_vector.shape}")
                print(f"First 10 dimensions: {sentence_vector[:10].round(3)}")
                print()
            else:
                print(f"Sentence {i+1}: '{sentence}' - No valid tokens found in vocabulary for embedding.")


# Test Word Embeddings
embedding_demo = WordEmbeddingsDemo()

# Training data (expanded for better embeddings)
training_docs = [
    "driver riding motorcycle without helmet",
    "vehicle speeding over limit on highway",
    "car jumping red signal at intersection",
    "motorcycle triple riding with passengers",
    "driver using mobile phone while driving",
    "riding without helmet protection gear",
    "speeding vehicle detected by camera",
    "signal jumping violation at traffic light",
    "three people riding on two wheeler",
    "mobile phone usage while operating vehicle",
    "helmet safety violation motorcycle riding",
    "high speed driving on city roads",
    "red light camera signal violation",
    "multiple riders on motorcycle helmetless",
    "texting talking phone driving distracted"
]

# Prepare training data
# Assuming word_tokenize is available from a previous cell or import
# from nltk.tokenize import word_tokenize # Uncomment if needed
# If word_tokenize is not available, you might need to explicitly import it or use a different tokenizer.
try:
    training_sentences = embedding_demo.prepare_training_data(training_docs)
except NameError:
    print("Error: 'word_tokenize' is not defined. Please ensure NLTK and its punkt tokenizer are imported and downloaded.")
    training_sentences = [] # Set to empty to avoid further errors


if training_sentences:
    print("Training Sentences Sample:")
    for i, sent in enumerate(training_sentences[:5]):
        print(f"  {i+1}: {sent}")

    # Train Word2Vec model
    w2v_model = embedding_demo.train_word2vec(training_sentences, vector_size=50)

    # Demonstrate word relationships
    embedding_demo.demonstrate_word_relationships(w2v_model)

    # Visualize embeddings
    traffic_words = ['helmet', 'riding', 'speeding', 'signal', 'mobile',
                    'vehicle', 'driver', 'motorcycle', 'violation', 'driving']
    embedding_demo.visualize_embeddings(w2v_model, traffic_words)

    # Sentence embeddings
    sample_sentences = [
        "riding without helmet on motorcycle",
        "speeding vehicle on highway",
        "jumping red signal at intersection"
    ]
    embedding_demo.demonstrate_sentence_embeddings(w2v_model, sample_sentences)

    # Using pre-trained embeddings (conceptual)
    print("\n" + "="*60)
    print("USING PRE-TRAINED EMBEDDINGS")
    print("="*60)

    print("""
    For production systems, consider using pre-trained embeddings:

    1. GloVe (Global Vectors for Word Representation)
    2. FastText
    3. BERT embeddings
    4. Word2Vec Google News

    These provide better semantic understanding and handle OOV words.
    """)

    # Example of loading pre-trained embeddings (conceptual)
    print("Conceptual code for pre-trained embeddings:")
    print("""
    # Load pre-trained GloVe embeddings
    def load_glove_embeddings(glove_path):
        embeddings_index = {}
        with open(glove_path, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                word = values[0]
                coefs = np.asarray(values[1:], dtype='float32')
                embeddings_index[word] = coefs
        return embeddings_index

    glove_embeddings = load_glove_embeddings('glove.6B.100d.txt')
    """)
else:
    print("Skipping model training and demonstration due to errors in data preparation.")

!pip install gensim