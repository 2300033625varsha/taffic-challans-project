# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import learning_curve
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from scipy.sparse import issparse # Import issparse
from sklearn.model_selection import cross_val_score # Import cross_val_score

print("\nðŸ”§ 11. MODEL EVALUATION AND INTERPRETATION")
print("="*50)

class ModelEvaluator:
    def __init__(self):
        self.results = {}

    def comprehensive_classification_report(self, y_true, y_pred, class_names):
        """Generate comprehensive classification report"""
        print("COMPREHENSIVE CLASSIFICATION REPORT")
        print("=" * 60)

        # Basic metrics
        accuracy = accuracy_score(y_true, y_pred)
        print(f"Overall Accuracy: {accuracy:.4f}")

        # Detailed classification report
        print("\nDetailed Classification Report:")
        print(classification_report(y_true, y_pred, target_names=class_names))

        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)

        print("\nPer-class Metrics:")
        print(f"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
        print("-" * 55)
        for i, class_name in enumerate(class_names):
            print(f"{class_name:<15} {precision[i]:<10.3f} {recall[i]:<10.3f} {f1[i]:<10.3f} {support[i]:<10}")

    def plot_confusion_matrix(self, y_true, y_pred, class_names):
        """Plot confusion matrix"""
        cm = confusion_matrix(y_true, y_pred)

        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=class_names, yticklabels=class_names)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.tight_layout()
        plt.show()

        return cm

    def plot_learning_curve(self, estimator, X, y, title="Learning Curve"):
        """Plot learning curve"""
        # Reduced cv to avoid ValueError with small sample dataset
        train_sizes, train_scores, test_scores = learning_curve(
            estimator, X, y, cv=2, n_jobs=-1, # Reduced cv to 2
            train_sizes=np.linspace(0.1, 1.0, 10),
            scoring='accuracy'
        )

        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        test_scores_std = np.std(test_scores, axis=1)

        plt.figure(figsize=(10, 6))
        plt.grid()
        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                        train_scores_mean + train_scores_std, alpha=0.1, color="r")
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                        test_scores_mean + test_scores_std, alpha=0.1, color="g")
        plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
        plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
        plt.xlabel("Training examples")
        plt.ylabel("Accuracy")
        plt.legend(loc="best")
        plt.title(title)
        plt.show()

    def feature_importance_analysis(self, model, feature_names, top_n=15):
        """Analyze feature importance"""
        if hasattr(model, 'coef_'):
            print("\nFEATURE IMPORTANCE ANALYSIS:")
            print("=" * 40)

            # Convert sparse coef_ to dense if necessary
            coef_dense = model.coef_.todense() if issparse(model.coef_) else model.coef_

            if len(coef_dense.shape) > 1 and coef_dense.shape[0] > 1:  # Multi-class
                for i, class_coef in enumerate(coef_dense):
                    print(f"\nClass {i} Top Features:")
                    # Ensure class_coef is a 1D array for argsort
                    class_coef_1d = np.asarray(class_coef).flatten()
                    top_indices = class_coef_1d.argsort()[-top_n:][::-1]
                    for idx in top_indices:
                        print(f"  {feature_names[idx]}: {class_coef_1d[idx]:.4f}")
            elif len(coef_dense.shape) > 1: # Binary case treated as multi-class with one row
                 print("\nTop Features (Binary Classification):")
                 class_coef_1d = np.asarray(coef_dense[0]).flatten()
                 top_indices = class_coef_1d.argsort()[-top_n:][::-1]
                 for idx in top_indices:
                        print(f"  {feature_names[idx]}: {class_coef_1d[idx]:.4f}")

            else:  # Binary classification with 1D coef_
                top_indices = coef_dense.argsort()[-top_n:][::-1]
                print("Top Features:")
                for idx in top_indices:
                    print(f"  {feature_names[idx]}: {coef_dense[idx]:.4f}")


    def model_comparison(self, models, X_test, y_test, model_names):
        """Compare multiple models"""
        print("\nMODEL COMPARISON")
        print("=" * 50)

        comparison_results = []

        for name, model in zip(model_names, models):
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

            comparison_results.append({
                'Model': name,
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1
            })

        # Create comparison table
        comparison_df = pd.DataFrame(comparison_results)
        print(comparison_df.round(4))

        # Plot comparison
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        comparison_df.set_index('Model')[metrics].plot(kind='bar', figsize=(12, 6))
        plt.title('Model Comparison')
        plt.ylabel('Score')
        plt.xticks(rotation=45)
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.show()

        return comparison_df

# Create sample data for evaluation
print("CREATING SAMPLE DATASET FOR EVALUATION...")

# Generate sample violation classification data
violation_data = {
    'text': [
        "riding without helmet on motorcycle", "no helmet while riding bike", "driver without protective helmet", "helmet not worn by rider", "rider without helmet", "wearing no helmet on bike",
        "speeding over limit on highway", "overspeeding vehicle detected", "high speed on city roads", "vehicle exceeding speed limit", "driving too fast", "exceeding speed limit",
        "jumping red signal at intersection", "signal violation at traffic light", "ran through red signal", "disregarded traffic signal", "ignoring red light", "running a red light",
        "triple riding on two wheeler", "three people on motorcycle", "multiple riders on scooter", "triple seat motorcycle riding", "three on a bike", "more than two riders",
        "using mobile phone while driving", "talking on phone during driving", "mobile usage while operating vehicle", "handheld device while driving", "texting while driving", "on phone while driving"
    ],
    'violation_type': [
        'Helmet', 'Helmet', 'Helmet', 'Helmet', 'Helmet', 'Helmet',
        'Speeding', 'Speeding', 'Speeding', 'Speeding', 'Speeding', 'Speeding',
        'Signal', 'Signal', 'Signal', 'Signal', 'Signal', 'Signal',
        'Triple', 'Triple', 'Triple', 'Triple', 'Triple', 'Triple',
        'Mobile', 'Mobile', 'Mobile', 'Mobile', 'Mobile', 'Mobile'
    ]
}

df = pd.DataFrame(violation_data)

# Prepare features and labels
vectorizer = TfidfVectorizer(max_features=50, stop_words='english')
X = vectorizer.fit_transform(df['text'])
y = df['violation_type']

# Encode labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
)

# Train multiple models for comparison
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

models = [
    LogisticRegression(random_state=42, max_iter=1000),
    MultinomialNB(),
    SVC(kernel='linear', random_state=42),
    RandomForestClassifier(n_estimators=100, random_state=42)
]

model_names = ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest']

print("TRAINING MODELS...")
trained_models = []
for model in models:
    model.fit(X_train, y_train)
    trained_models.append(model)

# Initialize evaluator
evaluator = ModelEvaluator()

# Evaluate each model
for name, model in zip(model_names, trained_models):
    print(f"\n{name} Evaluation:")
    print("=" * 40)

    y_pred = model.predict(X_test)

    # Comprehensive report
    evaluator.comprehensive_classification_report(
        y_test, y_pred, label_encoder.classes_
    )

    # Feature importance (for linear models)
    if hasattr(model, 'coef_'):
        evaluator.feature_importance_analysis(
            model, vectorizer.get_feature_names_out(), top_n=10
        )

# Plot confusion matrix for best model (Logistic Regression)
print("\nCONFUSION MATRIX FOR LOGISTIC REGRESSION:")
best_model = trained_models[0]
y_pred_best = best_model.predict(X_test)
cm = evaluator.plot_confusion_matrix(y_test, y_pred_best, label_encoder.classes_)

# Model comparison
print("\nCOMPREHENSIVE MODEL COMPARISON:")
comparison_df = evaluator.model_comparison(
    trained_models, X_test, y_test, model_names
)

# Learning curve for best model
print("\nLEARNING CURVE FOR LOGISTIC REGRESSION:")
evaluator.plot_learning_curve(
    LogisticRegression(max_iter=1000, random_state=42),
    X_train, y_train,
    "Learning Curve (Logistic Regression)"
)

# Error analysis
print("\nERROR ANALYSIS:")
print("=" * 40)

# Get misclassified examples
y_pred_all = trained_models[0].predict(X)  # Using first model
misclassified_indices = np.where(y_pred_all != y_encoded)[0]

print(f"Total misclassified: {len(misclassified_indices)}/{len(y_encoded)}")
print("\nSample misclassified examples:")

for idx in misclassified_indices[:5]:
    original_text = df.iloc[idx]['text']
    true_label = label_encoder.inverse_transform([y_encoded[idx]])[0]
    pred_label = label_encoder.inverse_transform([y_pred_all[idx]])[0]

    print(f"Text: {original_text}")
    print(f"True: {true_label}, Predicted: {pred_label}")
    print("-" * 60)

# Cross-validation scores
print("\nCROSS-VALIDATION RESULTS:")
print("=" * 40)

for name, model in zip(model_names, trained_models):
    # Reduced cv to avoid ValueError with small sample dataset
    cv_scores = cross_val_score(model, X, y_encoded, cv=2, scoring='accuracy') # Reduced cv to 2
    print(f"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

print("\n" + "="*60)
print("EVALUATION COMPLETE!")
print("="*60)