# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

print("\nðŸ”§ 3. TEXT PREPROCESSING PIPELINE")
print("="*50)

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
# Download the missing 'punkt_tab' resource
print("Attempting to download 'punkt_tab'...")
nltk.download('punkt_tab')
print("'punkt_tab' download attempt finished.")


class TextPreprocessor:
    def __init__(self):
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))

    def clean_text(self, text):
        """Remove special characters and digits"""
        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Convert to lowercase
        text = text.lower()
        # Remove extra whitespace
        text = ' '.join(text.split())
        return text

    def remove_stopwords(self, tokens):
        """Remove stopwords"""
        return [token for token in tokens if token not in self.stop_words]

    def stem_tokens(self, tokens):
        """Apply stemming"""
        return [self.stemmer.stem(token) for token in tokens]

    def lemmatize_tokens(self, tokens):
        """Apply lemmatization"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def preprocess_pipeline(self, text, use_stemming=True):
        """Complete preprocessing pipeline"""
        # Clean text
        cleaned_text = self.clean_text(text)

        # Tokenization
        tokens = word_tokenize(cleaned_text)

        # Remove stopwords
        tokens = self.remove_stopwords(tokens)

        # Apply stemming or lemmatization
        if use_stemming:
            tokens = self.stem_tokens(tokens)
        else:
            tokens = self.lemmatize_tokens(tokens)

        return ' '.join(tokens)

# Test the preprocessing pipeline
preprocessor = TextPreprocessor()

sample_texts = [
    "Driver was NOT wearing helmet while riding motorcycle!!!",
    "Vehicle exceeded speed limit by 40 km/h on highway.",
    "Car jumped red signal at Benz Circle intersection?",
    "Motorcycle with 3 passengers - triple riding violation!",
    "Driver using mobile phone (iPhone) while driving vehicle."
]

print("Original vs Preprocessed Text:")
print("-" * 80)
for i, text in enumerate(sample_texts):
    processed = preprocessor.preprocess_pipeline(text)
    print(f"Original {i+1}: {text}")
    print(f"Processed {i+1}: {processed}")
    print("-" * 80)