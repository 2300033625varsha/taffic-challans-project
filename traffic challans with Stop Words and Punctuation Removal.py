# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

import string
from collections import Counter

print("\nðŸ”§ 6. STOP WORDS AND PUNCTUATION REMOVAL")
print("="*50)

class TextCleaner:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.punctuation = set(string.punctuation)

        # Custom stop words for traffic domain
        self.custom_stop_words = {
            'vehicle', 'driver', 'road', 'street', 'way', 'area', 'zone'
        }

        self.all_stop_words = self.stop_words.union(self.custom_stop_words)

    def remove_punctuation(self, text):
        """Remove punctuation from text"""
        return text.translate(str.maketrans('', '', string.punctuation))

    def remove_stopwords(self, tokens):
        """Remove stopwords from token list"""
        return [token for token in tokens if token.lower() not in self.all_stop_words]

    def analyze_text(self, text):
        """Analyze text before and after cleaning"""
        print(f"Original Text: {text}")
        print("-" * 50)

        # Remove punctuation
        no_punct = self.remove_punctuation(text)
        print(f"Without Punctuation: {no_punct}")

        # Tokenize
        tokens = word_tokenize(no_punct.lower())
        print(f"Tokens: {tokens}")

        # Identify stopwords
        stopwords_found = [token for token in tokens if token in self.all_stop_words]
        print(f"Stopwords Found: {stopwords_found}")

        # Remove stopwords
        clean_tokens = self.remove_stopwords(tokens)
        print(f"Clean Tokens: {clean_tokens}")

        return clean_tokens

    def demonstrate_frequency_analysis(self, texts):
        """Show word frequency before and after stopword removal"""
        all_tokens = []
        all_clean_tokens = []

        for text in texts:
            # Original tokens
            tokens = word_tokenize(text.lower())
            tokens = [token for token in tokens if token not in self.punctuation]
            all_tokens.extend(tokens)

            # Clean tokens (no stopwords)
            clean_tokens = self.remove_stopwords(tokens)
            all_clean_tokens.extend(clean_tokens)

        # Frequency analysis
        original_freq = Counter(all_tokens)
        clean_freq = Counter(all_clean_tokens)

        print("\nWORD FREQUENCY ANALYSIS:")
        print("-" * 40)
        print(f"{'TOP 10 ORIGINAL':<25} {'TOP 10 CLEAN':<25}")
        print("-" * 40)

        for (orig_word, orig_count), (clean_word, clean_count) in zip(
            original_freq.most_common(10), clean_freq.most_common(10)):
            print(f"{orig_word:<15} ({orig_count:<2})        {clean_word:<15} ({clean_count:<2})")

# Test the text cleaner
cleaner = TextCleaner()

violation_descriptions = [
    "The driver was riding the motorcycle without a helmet on the main road",
    "A vehicle was speeding on the highway and was caught by police",
    "Car jumped the red signal at the busy intersection during peak hours",
    "Driver was using mobile phone while driving the vehicle on the street"
]

print("TEXT CLEANING DEMONSTRATION:")
print("="*60)

for i, text in enumerate(violation_descriptions, 1):
    print(f"\nExample {i}:")
    clean_tokens = cleaner.analyze_text(text)
    print(f"Final Clean Text: {' '.join(clean_tokens)}")
    print("="*60)

# Frequency analysis
cleaner.demonstrate_frequency_analysis(violation_descriptions)

# Custom stop words demonstration
print(f"\nCUSTOM STOP WORDS (Traffic Domain):")
print(f"Standard Stopwords: {len(cleaner.stop_words)}")
print(f"Custom Stopwords: {len(cleaner.custom_stop_words)}")
print(f"Total Stopwords: {len(cleaner.all_stop_words)}")
print(f"Sample Custom Stopwords: {list(cleaner.custom_stop_words)[:10]}")