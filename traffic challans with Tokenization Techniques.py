# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

print("\nðŸ”§ 4. TOKENIZATION TECHNIQUES")
print("="*50)

# Import Tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer
import nltk
from nltk.tokenize import word_tokenize

# Download NLTK punkt if not already downloaded
try:
    nltk.data.find('tokenizers/punkt')
except nltk.downloader.DownloadError:
    nltk.download('punkt')
except LookupError:
    nltk.download('punkt')


def demonstrate_tokenization(texts):
    """Demonstrate different tokenization techniques"""

    for i, text in enumerate(texts):
        print(f"\nText {i+1}: {text}")
        print("-" * 50)

        # Word Tokenization
        words = word_tokenize(text)
        print(f"Word Tokenization: {words}")

        # Character Tokenization
        chars = list(text)
        print(f"Character Tokenization (first 20): {chars[:20]}")

        # N-gram Tokenization
        def generate_ngrams(tokens, n):
            return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

        bigrams = generate_ngrams(words, 2)
        trigrams = generate_ngrams(words, 3)

        print(f"Bigrams: {bigrams[:5]}")
        print(f"Trigrams: {trigrams[:3]}")

        # Subword Tokenization (simulated)
        def subword_tokenization(word, min_length=3):
            if len(word) <= min_length:
                return [word]
            subwords = []
            for i in range(min_length, len(word)+1):
                subwords.append(word[:i])
            return subwords

        sample_word = "violation" if len(words) > 0 else "example"
        subwords = subword_tokenization(sample_word)
        print(f"Subword Tokenization of '{sample_word}': {subwords}")

# Test tokenization
sample_texts = [
    "Driver without helmet",
    "Speeding vehicle on highway",
    "Red signal jumping violation"
]

demonstrate_tokenization(sample_texts)

# Keras Tokenizer example
print("\n" + "="*50)
print("KERAS TOKENIZER IMPLEMENTATION")
print("="*50)

violation_corpus = [
    "riding without helmet on motorcycle",
    "speeding over limit on highway",
    "jumping red signal at intersection",
    "triple riding on two wheeler",
    "using mobile phone while driving"
]

keras_tokenizer = Tokenizer(num_words=50, oov_token="<OOV>")
keras_tokenizer.fit_on_texts(violation_corpus)

print("Word Index:", keras_tokenizer.word_index)
print("\nTexts to Sequences:")
for text in violation_corpus:
    sequence = keras_tokenizer.texts_to_sequences([text])[0]
    print(f"'{text}' -> {sequence}")