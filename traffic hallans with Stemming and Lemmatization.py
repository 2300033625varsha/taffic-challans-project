# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer

print("\nðŸ”§ 5. STEMMING AND LEMMATIZATION")
print("="*50)

class StemmingLemmatizationDemo:
    def __init__(self):
        self.porter = PorterStemmer()
        self.lancaster = LancasterStemmer()
        self.snowball = SnowballStemmer('english')
        self.lemmatizer = WordNetLemmatizer()

    def demonstrate_stemming(self, words):
        """Compare different stemming algorithms"""
        print("STEMMING COMPARISON:")
        print("-" * 60)
        print(f"{'Word':<15} {'Porter':<12} {'Lancaster':<12} {'Snowball':<12}")
        print("-" * 60)

        for word in words:
            porter_stem = self.porter.stem(word)
            lancaster_stem = self.lancaster.stem(word)
            snowball_stem = self.snowball.stem(word)

            print(f"{word:<15} {porter_stem:<12} {lancaster_stem:<12} {snowball_stem:<12}")

    def demonstrate_lemmatization(self, words):
        """Demonstrate lemmatization with different POS tags"""
        print("\nLEMMATIZATION WITH DIFFERENT POS TAGS:")
        print("-" * 50)
        print(f"{'Word':<15} {'Noun':<12} {'Verb':<12} {'Adjective':<12}")
        print("-" * 50)

        for word in words:
            noun_lemma = self.lemmatizer.lemmatize(word, pos='n')
            verb_lemma = self.lemmatizer.lemmatize(word, pos='v')
            adj_lemma = self.lemmatizer.lemmatize(word, pos='a')

            print(f"{word:<15} {noun_lemma:<12} {verb_lemma:<12} {adj_lemma:<12}")

    def compare_stem_lemma(self, words):
        """Compare stemming vs lemmatization"""
        print("\nSTEMMING vs LEMMATIZATION COMPARISON:")
        print("-" * 50)
        print(f"{'Word':<15} {'Porter Stem':<15} {'Lemma':<15}")
        print("-" * 50)

        for word in words:
            stem = self.porter.stem(word)
            lemma = self.lemmatizer.lemmatize(word)

            print(f"{word:<15} {stem:<15} {lemma:<15}")

# Test words related to traffic violations
test_words = [
    'riding', 'driving', 'speeding', 'jumping', 'using',
    'violation', 'detected', 'caught', 'observed', 'vehicle',
    'driver', 'motorcycle', 'helmet', 'signal', 'insurance'
]

demo = StemmingLemmatizationDemo()
demo.demonstrate_stemming(test_words)
demo.demonstrate_lemmatization(test_words[:8])  # First 8 words
demo.compare_stem_lemma(test_words)

# Practical example with sentences
print("\n" + "="*50)
print("PRACTICAL EXAMPLE WITH VIOLATION DESCRIPTIONS")
print("="*50)

violation_sentences = [
    "Driver was riding motorcycle without helmet",
    "Vehicle was speeding on the highway",
    "Car jumped the red signal at intersection",
    "Drivers were using mobile phones while driving"
]

for sentence in violation_sentences:
    # Tokenize and process each word
    words = word_tokenize(sentence.lower())
    filtered_words = [word for word in words if word.isalpha()]

    # Apply stemming
    stemmed_words = [demo.porter.stem(word) for word in filtered_words]

    # Apply lemmatization
    lemmatized_words = [demo.lemmatizer.lemmatize(word) for word in filtered_words]

    print(f"Original: {sentence}")
    print(f"Stemmed: {' '.join(stemmed_words)}")
    print(f"Lemmatized: {' '.join(lemmatized_words)}")
    print("-" * 60)