# -*- coding: utf-8 -*-
"""Untitled47.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D5GhSfL_h66prjEfLwvqJdGyLBX8zo_-
"""

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

print("\nðŸ”§ 8. BAG-OF-WORDS MODEL")
print("="*50)

class BagOfWordsDemo:
    def __init__(self):
        self.vectorizer = CountVectorizer()

    def demonstrate_bow(self, documents):
        """Demonstrate Bag-of-Words transformation"""
        print("DOCUMENT COLLECTION:")
        print("-" * 60)
        for i, doc in enumerate(documents, 1):
            print(f"Doc {i}: {doc}")

        # Create Bag-of-Words model
        X = self.vectorizer.fit_transform(documents)
        feature_names = self.vectorizer.get_feature_names_out()

        print(f"\nVOCABULARY ({len(feature_names)} words):")
        print(feature_names)

        # Convert to DataFrame for better visualization
        bow_df = pd.DataFrame(X.toarray(), columns=feature_names)

        print(f"\nBAG-OF-WORDS MATRIX:")
        print("Rows: Documents, Columns: Words, Values: Word Counts")
        print(bow_df)

        return X, feature_names, bow_df

    def demonstrate_ngrams(self, documents):
        """Demonstrate n-grams in Bag-of-Words"""
        print("\n" + "="*60)
        print("N-GRAMS IN BAG-OF-WORDS")
        print("="*60)

        # Unigrams
        uni_vectorizer = CountVectorizer(ngram_range=(1, 1))
        X_uni = uni_vectorizer.fit_transform(documents)
        uni_features = uni_vectorizer.get_feature_names_out()

        # Bigrams
        bi_vectorizer = CountVectorizer(ngram_range=(2, 2))
        X_bi = bi_vectorizer.fit_transform(documents)
        bi_features = bi_vectorizer.get_feature_names_out()

        # Unigrams + Bigrams
        uni_bi_vectorizer = CountVectorizer(ngram_range=(1, 2))
        X_uni_bi = uni_bi_vectorizer.fit_transform(documents)
        uni_bi_features = uni_bi_vectorizer.get_feature_names_out()

        print("UNIGRAMS (1-gram):")
        print(f"Features: {list(uni_features)}")

        print("\nBIGRAMS (2-gram):")
        print(f"Features: {list(bi_features)}")

        print(f"\nUNIGRAMS + BIGRAMS (1-2 grams):")
        print(f"Total Features: {len(uni_bi_features)}")
        print(f"Sample Features: {list(uni_bi_features)[:15]}")

    def demonstrate_parameters(self, documents):
        """Demonstrate different BOW parameters"""
        print("\n" + "="*60)
        print("BOW PARAMETER TUNING")
        print("="*60)

        # Different parameter configurations
        configurations = [
            {'max_features': 50, 'stop_words': 'english', 'ngram_range': (1, 1)},
            {'max_features': 100, 'stop_words': 'english', 'ngram_range': (1, 2)},
            {'max_df': 0.8, 'min_df': 2, 'stop_words': 'english'},
            {'binary': True, 'stop_words': 'english'}  # Binary BOW
        ]

        config_names = [
            "Limited Features (50) + Unigrams",
            "More Features (100) + Bigrams",
            "Document Frequency Filtering",
            "Binary Bag-of-Words"
        ]

        for config, name in zip(configurations, config_names):
            print(f"\n{name}:")
            print(f"Parameters: {config}")

            vectorizer = CountVectorizer(**config)
            X = vectorizer.fit_transform(documents)
            features = vectorizer.get_feature_names_out()

            print(f"Vocabulary Size: {len(features)}")
            print(f"Sample Features: {list(features)[:10]}")

# Test Bag-of-Words
bow_demo = BagOfWordsDemo()

# Sample violation documents
violation_docs = [
    "driver riding motorcycle without helmet",
    "vehicle speeding on highway over limit",
    "car jumping red signal at intersection",
    "motorcycle triple riding with passengers",
    "driver using mobile phone while driving"
]

# Basic BOW demonstration
X_bow, features, bow_df = bow_demo.demonstrate_bow(violation_docs)

# N-grams demonstration
bow_demo.demonstrate_ngrams(violation_docs)

# Parameter tuning demonstration
bow_demo.demonstrate_parameters(violation_docs)

# Practical application: Violation type classification
print("\n" + "="*60)
print("PRACTICAL APPLICATION: VIOLATION CLASSIFICATION WITH BOW")
print("="*60)

# Expanded dataset for classification
violation_data = {
    'text': [
        "riding without helmet on motorcycle",
        "speeding over limit on highway",
        "jumping red signal at intersection",
        "triple riding on two wheeler",
        "using mobile phone while driving",
        "no helmet while riding bike",
        "overspeeding vehicle detected",
        "signal violation at traffic light",
        "three people on motorcycle",
        "talking on phone during driving",
        "driver without protective helmet",
        "high speed on city roads",
        "ran through red signal",
        "multiple riders on scooter",
        "mobile usage while operating vehicle"
    ],
    'violation_type': [
        'Helmet', 'Speeding', 'Signal', 'Triple', 'Mobile',
        'Helmet', 'Speeding', 'Signal', 'Triple', 'Mobile',
        'Helmet', 'Speeding', 'Signal', 'Triple', 'Mobile'
    ]
}

df = pd.DataFrame(violation_data)

# Create BOW features
vectorizer = CountVectorizer(max_features=50, stop_words='english')
X = vectorizer.fit_transform(df['text'])
y = df['violation_type']

print("Violation Classification Dataset:")
print(df)
print(f"\nBOW Feature Matrix Shape: {X.shape}")
print(f"Feature Names: {vectorizer.get_feature_names_out()}")